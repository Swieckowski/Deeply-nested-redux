# Deeply-nested-redux
My core assumption going in is that for whatever reason our API returns deeply nested JSON data. Since you emphasized speed as the major hurdle to overcome, I considered two main options going forward. The first option is to use this object directly for our initial state, and the second is to normalize that data for our local state. 

The first approach has the advantage of a low initial time cost of loading the nested JSON data. However, testing on my versions of chrome and safari, reading a deep reference in an object is consistently a bit costlier (at rates of 3-8%) than reading a shallow reference, both from a cache and a flat object [https://jsperf.com/deep-reference-access-time]. But the biggest problems with this approach lie in code readability and time cost of making changes to state - as seen in my unwieldy updateDeepField function. And it only spirals from there once you consider the many different sorts of changes we can expect to need. For example, data moving up and down nested levels; add and remove functions quite similar to updateDeepField for each level would be required and used in tandem to achieve this. 

The second approach has a much higher initial time cost, and would require either a library like normalizr or an inhouse equivalent. Normalizing flattens our JSON into schemas we define. Just like normalization in a relational database, our resulting data should only be stored once, keyed by ID, and hierarchies represented by referencing to ID.
This leads to a much more manageable state for redux while preserving the relationships we want, as seen in the normalized reducer. 
